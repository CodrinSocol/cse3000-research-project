{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf72ff6",
   "metadata": {},
   "source": [
    "# Experiment 2 - Ablation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4d61c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891a3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6278f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract exp3 parameters (betas) from the df_total.gz file, obtained from LCDB\n",
    "\n",
    "pd.set_option('use_inf_as_na',True)\n",
    "df_total = pd.read_pickle('data/df_total.gz').dropna()\n",
    "\n",
    "data = df_total[(df_total['curve_model'] == 'exp3')]\n",
    "data = data.loc[data.groupby(['openmlid', 'learner'])['max_anchor_seen'].idxmax()]\n",
    "\n",
    "q50 = data['MSE_tst_last'].quantile(0.5)\n",
    "data = data[data['MSE_tst_last'] < q50]\n",
    "\n",
    "x = [x[0] for x in data['beta']]\n",
    "y = [x[1] for x in data['beta']]\n",
    "z = [x[2] for x in data['beta']]\n",
    "\n",
    "betas = pd.DataFrame(zip(x,y,z), columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faddb8",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a66aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_params(idx):\n",
    "    a = betas['a'].iloc[idx]\n",
    "    b = betas['b'].iloc[idx]\n",
    "    c = betas['c'].iloc[idx]\n",
    "    \n",
    "    return a,b,c\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    n = int(len(betas)) # number of LCs considered\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    asc = 0\n",
    "    desc = 0\n",
    "    \n",
    "    for i in tqdm(range(0,n)):\n",
    "        is_asc = False\n",
    "        flag = False\n",
    "        final_slopes = []\n",
    "        \n",
    "        lern = data['learner'].iloc[i]\n",
    "        dataset = data['openmlid'].iloc[i]\n",
    "        \n",
    "        a,b,c = gen_params(i)\n",
    "\n",
    "        exp3 = lambda x: a * np.exp((-b) * x) + c # exp3 from LCDB\n",
    "        anch = data['anchor_prediction'].iloc[i] # array of anchors\n",
    "        \n",
    "        if (a < 0 and b > 0) or (a > 0 and b < 0):\n",
    "            is_asc = True\n",
    "            asc+=1\n",
    "        else:\n",
    "            is_asc = False\n",
    "            desc+=1\n",
    "        \n",
    "        for j in range(0, len(anch)):\n",
    "            points = range(anch[j] - 10, anch[j] + 11)\n",
    "            errors_iterations = []\n",
    "            slopes = []\n",
    "            for it in range(0, 25):\n",
    "                noise = np.random.normal(0,0.002)\n",
    "                linreg_errs = [exp3(p)+noise for p in points]\n",
    "        \n",
    "                model = LinearRegression()\n",
    "                model.fit(np.array(points).reshape(-1,1), np.array(linreg_errs).reshape(-1,1))\n",
    "                \n",
    "                if(model.coef_[0] > 0):\n",
    "                    slopes.append(1)\n",
    "                else:\n",
    "                    slopes.append(-1)\n",
    "            final_slopes.append(np.mean(slopes))\n",
    "         \n",
    "        for j in range(0, len(anch)):\n",
    "            if final_slopes[j] > 0:\n",
    "                if is_asc:\n",
    "                    true_pos+=1\n",
    "                else:\n",
    "                    false_pos+=1\n",
    "                flag = True\n",
    "                break\n",
    "                \n",
    "        if flag == False:\n",
    "            if is_asc:\n",
    "                false_neg+=1\n",
    "            else:\n",
    "                true_neg+=1\n",
    "\n",
    "\n",
    "    print(f\"Out of {n} Learning curves:\")\n",
    "    print(f\"-----------{asc} Non-monotonic LCs\")\n",
    "    print(f\"-----------{desc} Monotonic LCs\\n\")\n",
    "    print(f\" - {true_pos} Have been classified correctly as non-monotonic ({round((true_pos/asc)*100,2)}%)\")\n",
    "    print(f\" - {true_neg} Have been classified correctly as monotonic ({round((true_neg/desc)*100,2)}%)\")\n",
    "    print(f\" - {false_pos} Have been classified incorrectly as non-monotonic (Type I Error)\")\n",
    "    print(f\" - {false_neg} Have been classified incorrectly as monotonic (Type II Error)\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999eab33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3769/3769 [09:39<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 3769 Learning curves:\n",
      "-----------3179 Non-monotonic LCs\n",
      "-----------590 Monotonic LCs\n",
      "\n",
      " - 3167 Have been classified correctly as non-monotonic (99.62%)\n",
      " - 355 Have been classified correctly as monotonic (60.17%)\n",
      " - 235 Have been classified incorrectly as non-monotonic (Type I Error)\n",
      " - 12 Have been classified incorrectly as monotonic (Type II Error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46a65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_lcs = [] # contains indexes from data DataFrame of the monotonic LCs.\n",
    "\n",
    "for i in range(0, len(betas)):\n",
    "    a,b,c = gen_params(i)\n",
    "    if (a > 0 and b > 0) or (a < 0 and b < 0):\n",
    "        monotonic_lcs.append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7a1b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaking_experiment():\n",
    "    n = len(monotonic_lcs)\n",
    "    \n",
    "    corr_nonmon_abl = 0 # correctly classified as non-monotonic when 1 anchor point is considered (ablation)\n",
    "    corr_nonmon_norm = 0 # correctly classified as non-monotonic when 2 anchor point are considered (initial metric)\n",
    "    \n",
    "    for i in tqdm(range(0, n)):\n",
    "        final_slopes = []\n",
    "        idx = monotonic_lcs[i]\n",
    "        a,b,c = gen_params(idx)\n",
    "        lern = data['learner'].iloc[idx]\n",
    "        dataset = data['openmlid'].iloc[idx]\n",
    "        \n",
    "        exp3 = lambda x: a * np.exp((-b) * x) + c # exp3 from LCDB\n",
    "        anch = data['anchor_prediction'].iloc[idx] # array of anchors\n",
    "        \n",
    "        for j in range(0, len(anch)):\n",
    "            points = range(anch[j] - 10, anch[j] + 11)\n",
    "            errors_iterations = []\n",
    "            slopes = []\n",
    "            for it in range(0, 25):\n",
    "                noise = np.random.normal(0,0.002)\n",
    "                linreg_errs = None\n",
    "                if j == 5:\n",
    "                    linreg_errs = [exp3(j-1)+abs(noise) for p in points]\n",
    "                else:\n",
    "                    linreg_errs = [exp3(p)+noise for p in points]\n",
    "                \n",
    "                model = LinearRegression()\n",
    "                model.fit(np.array(points).reshape(-1,1), np.array(linreg_errs).reshape(-1,1))\n",
    "                \n",
    "                if(model.coef_[0] > 0):\n",
    "                    slopes.append(1)\n",
    "                else:\n",
    "                    slopes.append(-1)\n",
    "            final_slopes.append(np.mean(slopes))\n",
    "         \n",
    "        for j in range(0, len(anch)):\n",
    "            if final_slopes[j] > 0:\n",
    "                corr_nonmon_abl += 1\n",
    "                if j < len(anch) - 1 and final_slopes[j+1] > 0:\n",
    "                    corr_nonmon_norm += 1\n",
    "                break\n",
    "            \n",
    "    print(f\"Out of {n} monotonic LCs with peaking at anchor 5:\")\n",
    "    print(f\" - {corr_nonmon_abl} have been correctly classified as non-monotonic on ablation ({round((corr_nonmon_abl/n)*100,2)}%)\")\n",
    "    print(f\" - {corr_nonmon_norm} have been correctly classified as non-monotonic on normal metric ({round((corr_nonmon_norm/n)*100,2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daa8b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 590/590 [02:02<00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 590 monotonic LCs with peaking at anchor 5:\n",
      " - 250 have been correctly classified as non-monotonic on ablation (42.37%)\n",
      " - 22 have been correctly classified as non-monotonic on normal metric (3.73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "peaking_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
